# ia_services.py - CORRIGIDO
import os
import re
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

class IAService:
    def __init__(self):
        self.groq_key = os.environ.get("GROQ_API_KEY")
        self.use_real_ia = bool(self.groq_key)
        self.conversation_threads = {}  # ‚úÖ Threads de conversa√ß√£o por usu√°rio
        
        if self.use_real_ia:
            self.client = OpenAI(
                api_key=self.groq_key,
                base_url="https://api.groq.com/openai/v1"
            )
            print("‚úÖ IA Groq - Conversa√ß√£o Cont√≠nua Ativa")
        else:
            print("‚ö†Ô∏è  Modo MOCK - Conversa√ß√£o Cont√≠nua")

    def dev_chat(self, messages, user_id="dev", action_type=None):
        """Chat Dev com conversa√ß√£o cont√≠nua - MELHORADO"""
        # ‚úÖ Mant√©m thread de conversa√ß√£o
        thread_id = f"dev_{user_id}"
        if thread_id not in self.conversation_threads:
            self.conversation_threads[thread_id] = []
        
        # Adiciona nova mensagem ao thread
        if messages:
            self.conversation_threads[thread_id].extend(messages[-2:])  # Mant√©m √∫ltimas 2
        
        # Limita o tamanho do thread
        if len(self.conversation_threads[thread_id]) > 20:
            self.conversation_threads[thread_id] = self.conversation_threads[thread_id][-20:]
        
        # Busca an√°lise REAL do c√≥digo
        from code_analyzer import code_analyzer
        analise_real = code_analyzer.analyze_project()
        
        # Prepara contexto inteligente
        contexto_acao = self._get_continuous_context(action_type, messages, analise_real, thread_id)
        
        system_context = f"""
        VOC√ä √â UM ENGENHEIRO ESPECIALISTA EM CONVERSA√á√ïES T√âCNICAS CONT√çNUAS.

        CONTEXTO DO SISTEMA:
        {self._format_real_analysis(analise_real)}

        {contexto_acao}

        SEU ESTILO DE CONVERSA:
        - Mantenha a conversa fluida e cont√≠nua
        - N√£o "feche" os t√≥picos - sempre permita continua√ß√£o
        - Use perguntas ret√≥ricas para engajar
        - Ofere√ßa aprofundamento natural
        - Seja t√©cnico mas acess√≠vel

        EXEMPLOS DE RESPOSTAS CONT√çNUAS:
        "Analisando esse aspecto mais a fundo..." 
        "Quer que eu detalhe alguma parte espec√≠fica?"
        "Vamos explorar isso melhor..."
        "Algum ponto em particular voc√™ gostaria de expandir?"

        EVITE:
        - "Passo 1, 2, 3..." (muito rob√≥tico)
        - Encerrar t√≥picos abruptamente
        - Listas muito longas sem engajamento
        """

        # Usa o thread completo para contexto
        full_messages = self.conversation_threads[thread_id].copy()
        
        return self._continuous_groq_chat(full_messages, system_context, action_type)

    def _get_continuous_context(self, action_type, messages, analise_real, thread_id):
        """Contexto para conversa√ß√£o cont√≠nua - MELHORADO"""
        if not action_type:
            return "Continue a conversa naturalmente, permitindo aprofundamento nos t√≥picos."
        
        # Verifica se √© continua√ß√£o de a√ß√£o anterior
        last_action = getattr(self, f'_last_action_{thread_id}', None)
        setattr(self, f'_last_action_{thread_id}', action_type)

        
        action_contexts = {
            "detailed_analysis": f"""
            CONTINUA√á√ÉO DA AN√ÅLISE DETALHADA:
            {f'Continua√ß√£o do t√≥pico anterior: {last_action}' if last_action == 'detailed_analysis' else 'Iniciando an√°lise detalhada...'}
            
            Mantenha a an√°lise fluida e aprofund√°vel. 
            Convide para explorar aspectos espec√≠ficos.
            Ofere√ßa diferentes √¢ngulos de an√°lise.
            """,
            
            "debug": f"""
            CONTINUA√á√ÉO DO DEBUG:
            {f'Continuando debug do problema anterior' if last_action == 'debug' else 'Iniciando an√°lise de debug...'}
            
            Explore os problemas de forma conversacional.
            Pe√ßa mais contexto se necess√°rio.
            Sugira pr√≥ximos passos naturalmente.
            """,
            
            "practical_example": f"""
            CONTINUA√á√ÉO DO EXEMPLO PR√ÅTICO:
            {f'Expandindo o exemplo anterior' if last_action == 'practical_example' else 'Criando exemplo pr√°tico...'}
            
            Desenvolva o exemplo de forma incremental.
            Pe√ßa feedback sobre a implementa√ß√£o.
            Ofere√ßa varia√ß√µes do exemplo.
            """,
            
            "performance": f"""
            CONTINUA√á√ÉO DA AN√ÅLISE DE PERFORMANCE:
            {f'Aprofundando an√°lise de performance' if last_action == 'performance' else 'Iniciando an√°lise de performance...'}
            
            Explore m√©tricas de forma conversacional.
            Compare diferentes abordagens.
            Pe√ßa contexto sobre casos de uso espec√≠ficos.
            """
        }
        
        return action_contexts.get(action_type, "Continue a conversa t√©cnica de forma natural.")

    def _continuous_groq_chat(self, messages, system_context, action_type):
        """Chat com foco em continuidade - NOVO"""
        if not self.use_real_ia:
            return self._continuous_mock_response(messages, system_context, action_type)
        
        try:
            system_msg = {"role": "system", "content": system_context}
            full_messages = [system_msg] + messages
            
            response = self.client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=full_messages,
                temperature=0.7,  # Mais criativo para conversa√ß√£o
                max_tokens=600,
                timeout=15
            )
            
            content = response.choices[0].message.content
            return self._format_continuous_response(content, action_type)
            
        except Exception as e:
            print(f"‚ùå Erro Groq: {e}")
            return self._continuous_mock_response(messages, system_context, action_type)

    def _format_continuous_response(self, text, action_type):
        """Formata resposta para incentivar continuidade - NOVO"""
        if not text:
            return "Gostaria de explorar mais algum aspecto espec√≠fico?"
        
        # Remove fechamentos muito definitivos
        text = re.sub(r'(em conclus√£o|finalmente|para resumir|em suma)[^.!?]*[.!?]', '', text, flags=re.IGNORECASE)
        
        # Adiciona engajamento se a resposta for muito "fechada"
        if not any(word in text.lower() for word in ['?', 'quer', 'gostaria', 'vamos', 'explorar', 'detalhar']):
            engagement_phrases = [
                "\n\nO que achou dessa abordagem?",
                "\n\nQuer que eu detalhe algum ponto espec√≠fico?",
                "\n\nVamos explorar isso mais a fundo?",
                "\n\nAlguma parte em particular te interessa mais?"
            ]
            
            import random
            text += random.choice(engagement_phrases)
        
        # Limpa HTML
        text = re.sub(r'</?strong>', '**', text)
        text = re.sub(r'</?code>', '`', text)
        text = re.sub(r'<[^>]+>', '', text)
        
        return text

    def _continuous_mock_response(self, messages, system_context, action_type):
        """Mock responses que incentivam continuidade - MELHORADO"""
        last_msg = messages[-1]['content'].lower() if messages else ""
        
        if "dev" in system_context.lower():
            continuous_responses = {
                "detailed_analysis": [
                    "üîç **An√°lise em Andamento:** Identifiquei alguns padr√µes interessantes na arquitetura. Quer que eu foque em algum componente espec√≠fico como o sistema de autentica√ß√£o ou a estrutura do banco?",
                    
                    "üìä **Profundizando An√°lise:** Analisando o tratamento de erros, vejo oportunidades em auth.py. Como voc√™ lida atualmente com falhas de autentica√ß√£o? Podemos explorar isso.",
                    
                    "üîÑ **Continua√ß√£o da An√°lise:** Vamos olhar para a performance das queries SQLite? Ou prefere focar na escalabilidade da API? Me diga qual aspecto te interessa mais."
                ],
                
                "debug": [
                    "üêõ **Debug Cont√≠nuo:** Encontrei alguns pontos de melhoria no tratamento de exce√ß√µes. Quer que eu mostre como implementar logs mais detalhados?",
                    
                    "üîß **Aprofundando Debug:** Analisando o fluxo de autentica√ß√£o, h√° oportunidades para melhorar a valida√ß√£o. Como voc√™ monitora tentativas de login atualmente?",
                    
                    "‚ö° **Debug em Progresso:** Vamos examinar o consumo de mem√≥ria do sistema? Ou prefere focar em otimiza√ß√£o de consultas? Me oriente sobre sua prioridade."
                ],
                
                "practical_example": [
                    "üìù **Exemplo Expandido:** Aqui est√° uma implementa√ß√£o b√°sica. Quer que eu adicione tratamento de erros ou prefere ver uma vers√£o com cache?",
                    
                    "üöÄ **Desenvolvendo Exemplo:** Vamos evoluir esse c√≥digo juntos? Posso mostrar como adicionar logging, m√©tricas ou testes unit√°rios. O que te interessa?",
                    
                    "üí° **Exemplo em Camadas:** Esta √© a vers√£o simples. Quer ver como escalar para produ√ß√£o com rate limiting e monitoramento?"
                ],
                
                "performance": [
                    "‚ö° **An√°lise de Performance Cont√≠nua:** Identifiquei oportunidades em consultas SQL. Quer que eu mostre como adicionar √≠ndices ou prefere otimiza√ß√£o de conex√µes?",
                    
                    "üìà **M√©tricas em Foco:** Vamos explorar m√©tricas espec√≠ficas? Posso ajudar com monitoramento de tempo de resposta, throughput ou uso de recursos. Qual sua necessidade?",
                    
                    "üîç **Performance Profunda:** Analisando o sistema, vejo potencial em cache. Quer implementar cache em mem√≥ria ou prefere focar em otimiza√ß√£o de algoritmos?"
                ]
            }
            
            import random
            responses = continuous_responses.get(action_type, [
                "üí≠ Interessante! Como posso ajudar voc√™ a explorar isso mais a fundo?",
                "üîç Vamos continuar essa an√°lise? Em que aspecto espec√≠fico voc√™ gostaria de se aprofundar?",
                "üöÄ √ìtimo ponto! Quer que eu detalhe alguma parte espec√≠fica ou explore uma abordagem diferente?"
            ])
            
            return random.choice(responses)
        else:
            # Respostas cont√≠nuas para usu√°rio
            return "ü§ñ Como posso continuar ajudando voc√™? Tem alguma d√∫vida espec√≠fica ou quer explorar outra funcionalidade?"
    def user_chat(self, messages, user_id="user"):
        """Chat Usu√°rio Inteligente - CORRIGIDO"""
        system_context = """
        VOC√ä √â UM ASSISTENTE DO CHAT SYSTEM.

        SOBRE ESTE SISTEMA:
        - Site de chat com IA gratuita
        - Dois widgets: üí¨ (ajuda geral) e üîß (an√°lise t√©cnica)
        - Desenvolvido com FastAPI, SQLite e JavaScript
        - IA integrada com Groq API

        SUAS FUN√á√ïES:
        - Explicar como o sistema funciona
        - Ajudar a usar os recursos dispon√≠veis
        - Responder perguntas t√©cnicas b√°sicas
        - Direcionar para an√°lise t√©cnica quando necess√°rio

        CREDENCIAIS DEV: admin / admin123

        ESTILO:
        - Respostas √∫teis e diretas (100-200 palavras)
        - Tom amig√°vel e profissional
        - Use emojis moderadamente
        - Ofere√ßa ajuda adicional
        """

        return self._smart_groq_chat(messages, system_context)

    def _get_smart_action_context(self, action_type, messages, analise_real):
        """Gera contexto inteligente para a√ß√µes - CORRIGIDO"""
        if not action_type:
            return "Forne√ßa ajuda t√©cnica geral baseada na conversa."
        
        # Pega a √∫ltima mensagem do usu√°rio para contexto
        ultima_user_msg = ""
        for msg in reversed(messages):
            if msg['role'] == 'user':
                ultima_user_msg = msg['content']
                break
        
        action_contexts = {
            "detailed_analysis": f"""
            O usu√°rio solicitou uma AN√ÅLISE DETALHADA.
            
            Contexto da conversa: {ultima_user_msg[:200] if ultima_user_msg else "Conversa geral"}
            
            Forne√ßa uma an√°lise t√©cnica aprofundada incluindo:
            - Arquitetura e estrutura do c√≥digo
            - Problemas identificados na an√°lise real
            - Recomenda√ß√µes de melhoria
            - Impacto nas funcionalidades
            """,
            
            "debug": f"""
            O usu√°rio solicitou DEBUG t√©cnico.
            
            Contexto: {ultima_user_msg[:200] if ultima_user_msg else "Sistema geral"}
            
            Foque em:
            - Identificar e explicar poss√≠veis bugs
            - Sugerir corre√ß√µes espec√≠ficas
            - Melhorar tratamento de erros
            - Logs e monitoramento
            """,
            
            "practical_example": f"""
            O usu√°rio solicitou um EXEMPLO PR√ÅTICO.
            
            Contexto: {ultima_user_msg[:200] if ultima_user_msg else "Implementa√ß√£o geral"}
            
            Forne√ßa:
            - C√≥digo implement√°vel e test√°vel
            - Exemplo concreto relacionado ao contexto
            - Explica√ß√£o passo a passo
            - Boas pr√°ticas aplicadas
            """,
            
            "performance": f"""
            O usu√°rio solicitou an√°lise de PERFORMANCE.
            
            Contexto: {ultima_user_msg[:200] if ultima_user_msg else "Otimiza√ß√£o geral"}
            
            Analise:
            - Poss√≠veis gargalos de performance
            - Otimiza√ß√µes espec√≠ficas
            - Melhorias de consulta e cache
            - M√©tricas e monitoramento
            """
        }
        
        return action_contexts.get(action_type, "Forne√ßa ajuda t√©cnica geral.")

    def _format_real_analysis(self, analise):
        """Formata a an√°lise real para contexto"""
        partes = []
        
        if analise['duplicate_functions']:
            for dup in analise['duplicate_functions'][:2]:
                if dup['function'] != '__init__':  # Ignora __init__
                    partes.append(f"üö® {dup['function']} em {dup['file1']} e {dup['file2']}")
        
        if analise['error_handling']:
            for err in analise['error_handling'][:2]:
                partes.append(f"‚ö†Ô∏è {err['file']}: {err['issue']}")
        
        if analise['security_issues']:
            for sec in analise['security_issues'][:2]:
                partes.append(f"üîí {sec['file']}: {sec['issue']}")
        
        return "\n".join(partes) if partes else "‚úÖ C√≥digo est√°vel"

    def _smart_groq_chat(self, messages, system_context):
        """Chat inteligente com fallback elegante - CORRIGIDO"""
        if not self.use_real_ia:
            return self._smart_mock_response(messages, system_context)
        
        try:
            system_msg = {"role": "system", "content": system_context}
            full_messages = [system_msg] + messages
            
            response = self.client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=full_messages,
                temperature=0.3,
                max_tokens=800,
                timeout=15
            )
            
            content = response.choices[0].message.content
            return self._clean_response(content)
            
        except Exception as e:
            print(f"‚ùå Erro Groq: {e}")
            return self._smart_mock_response(messages, system_context)

    def _smart_mock_response(self, messages, system_context):
        """Resposta mock inteligente - CORRIGIDO"""
        ultima_msg = messages[-1]['content'].lower() if messages else ""
        
        if "dev" in system_context.lower():
            # Respostas mock melhoradas baseadas no contexto
            if "an√°lise detalhada" in ultima_msg or "detailed_analysis" in str(messages):
                return "üîç **An√°lise Detalhada:** O sistema mostra boa arquitetura. Melhore o tratamento de erros em auth.py. Configure GROQ_API_KEY para an√°lise completa."
            
            elif "debug" in ultima_msg:
                return "üêõ **Debug:** Verifique fun√ß√µes sem tratamento de erro em auth.py. Logs ajudariam no monitoramento. Configure GROQ_API_KEY para debug detalhado."
            
            elif "exemplo pr√°tico" in ultima_msg or "practical_example" in str(messages):
                return "üìù **Exemplo Pr√°tico:** ```python\n# Melhoria em auth.py\ntry:\n    user = get_user(username)\nexcept Exception as e:\n    logger.error(f'Auth error: {e}')\n    return None\n```"
            
            elif "performance" in ultima_msg:
                return "‚ö° **Performance:** SQLite √© adequado para testes. Para produ√ß√£o, considere PostgreSQL. Otimize consultas frequentes."
            
            else:
                return "üí° Configure GROQ_API_KEY no .env para respostas t√©cnicas completas e contextualizadas."
        else:
            # Respostas para usu√°rio
            if "funcion" in ultima_msg or "como" in ultima_msg:
                return "ü§ñ Este √© um sistema de chat com IA gratuita. Use üí¨ para ajuda geral ou üîß para an√°lise t√©cnica (login: admin/admin123)."
            elif "ajuda" in ultima_msg or "help" in ultima_msg:
                return "üí¨ Posso ajudar! Este sistema permite conversar com IA. Para quest√µes t√©cnicas, use o widget üîß."
            else:
                return "Ol√°! Sou o assistente deste sistema de chat. Como posso ajudar?"

    def _clean_response(self, text):
        """Limpa resposta mantendo qualidade"""
        if not text: 
            return "Resposta n√£o dispon√≠vel."
        
        # Remove HTML mas mant√©m formata√ß√£o
        text = re.sub(r'</?strong>', '**', text)
        text = re.sub(r'</?code>', '`', text)
        text = re.sub(r'<[^>]+>', '', text)
        
        # Limita tamanho mas preserva estrutura
        if len(text) > 1000:
            paragraphs = text.split('\n\n')
            if len(paragraphs) > 3:
                text = '\n\n'.join(paragraphs[:3]) + "\n\n..."
        
        return text.strip()

# Inst√¢ncia global
ia_service = IAService()